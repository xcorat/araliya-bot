# Bus Protocol

**Status:** Implemented — `supervisor/bus.rs`, `supervisor/dispatch.rs`

The supervisor event bus is the communication channel between subsystems. This document specifies the protocol every subsystem participant must follow.

Supervisor management/control commands are intentionally outside this protocol and live on the supervisor-internal control plane (`supervisor/control.rs`).

---

## Design basis

The protocol follows **JSON-RPC 2.0 semantics** — request/response correlation by `id`, structured error objects with numeric codes, and a clear separation between requests (expecting a reply) and notifications (fire-and-forget). The in-process implementation uses Tokio channels; the type definitions are IPC-ready without callsite changes (see [IPC migration path](#ipc-migration-path)).

### Why a central bus and not direct calls?

The bus adds ~100–500 ns per request hop — negligible compared to the I/O it orchestrates (an LLM call is ~200 ms, a Gmail API call ~500 ms). Replies travel back via `oneshot` channels that bypass the supervisor entirely, so a full request–reply chain (e.g. PTY → agents → tools → agents → PTY) costs only 2 supervisor hops, not 4.

The star topology was chosen over alternatives for these reasons:

| Alternative | Why not |
|---|---|
| Direct function calls | Tight coupling; no centralised logging, cancellation, or permission enforcement |
| Actor-per-entity (Actix / Erlang) | Better at thousands of independent actors; we have <20 subsystems — a hub is simpler |
| Broker (NATS, Kafka) | 100–1000× slower (network + serialisation + persistence); designed for distributed systems we don't need yet |
| gRPC / microservices | Full network stack per call (~1–5 ms); premature for a single-process bot |

The single-threaded supervisor loop can dispatch ~2–5 M msgs/sec; this only becomes a bottleneck at scales far beyond the current design. If it ever does, the [IPC migration path](#ipc-migration-path) lets us shard without changing caller code.

---

## Method naming

Method strings use `/`-separated path segments:

```
"subsystem/component/action"
```

Examples:
- `"agents"` — agents subsystem, default component, default action
- `"agents/echo/handle"` — explicit agent + action
- `"llm/complete"` — LLM subsystem, complete action

**Reserved prefix:** `$/` is reserved for system-level events (e.g. `"$/cancel"`). No subsystem may register a handler with a prefix beginning with `$`.

The supervisor dispatches by the **first segment only**. Everything after the first `/` is passed verbatim to the handler for secondary routing. So `"agents/echo/handle"` is delivered to the `"agents"` handler, which sees the full method string and routes internally.

---

## Message kinds

```rust
pub enum BusMessage {
    Request {
        id: Uuid,
        method: String,
        payload: BusPayload,
        reply_tx: oneshot::Sender<BusResult>,
    },
    Notification {
        method: String,
        payload: BusPayload,
    },
}
```

### Request

- Caller awaits exactly one reply via the embedded `oneshot::Sender<BusResult>`.
- `reply_tx` is `!Clone` — single-recipient delivery is enforced at compile time.
- `id` is a `Uuid` generated by `BusHandle::request`; becomes the wire `id` field when IPC is added.
- Handlers **must not block the supervisor loop** — resolve `reply_tx` synchronously or move it into a `tokio::spawn` task.

### Notification

- Fire-and-forget. No reply channel, no `id`.
- Sent via `BusHandle::notify`, which uses `try_send` (non-blocking).
- **Intentionally lossy under back-pressure**: if the bus buffer is full the notification is dropped and `BusCallError::Full` is returned. Callers must log a warning and not retry. If guaranteed delivery is needed, use a `Request` instead.

---

## Payload enum

All known message bodies are variants of `BusPayload`. Every variant derives `Serialize + Deserialize` — the enum is IPC-ready without any callsite changes.

```rust
pub enum BusPayload {
    CommsMessage  { channel_id: String, content: String, session_id: Option<String> },
    LlmRequest    { channel_id: String, content: String },
    CancelRequest { id: Uuid },
    SessionQuery  { session_id: String },
    JsonResponse  { data: String },
    Empty,
}
```

`channel_id` is threaded through `LlmRequest` so the LLM subsystem can re-attach it to the `CommsMessage` reply, enabling callers to correlate replies with the originating channel without extra bookkeeping.

`session_id` on `CommsMessage` threads session identity end-to-end: comms channels send an optional `session_id` inbound, agents attach the memory session id on the reply, and the HTTP API returns it to the client.

`SessionQuery` / `JsonResponse` support structured subsystem queries (e.g. session list, session detail) without overloading `CommsMessage`.

When adding a new message type, add a new variant here. Do not reuse an existing variant for a semantically different message.

---

## Error codes

```rust
pub struct BusError {
    pub code: i32,
    pub message: String,
}

pub const ERR_METHOD_NOT_FOUND: i32 = -32601;  // mirrors JSON-RPC 2.0
```

`BusError` mirrors the JSON-RPC 2.0 error object. `ERR_METHOD_NOT_FOUND` (`-32601`) is returned by the supervisor when no handler is registered for the incoming method prefix. Application-level errors use the range `-32000` to `-32099` (JSON-RPC 2.0 server-defined errors).

---

## `BusHandle` API

`BusHandle` is the **only public surface** subsystems and plugins touch. Raw channel types are not exposed outside `bus.rs`. It is `Clone` and inexpensive to pass around.

```rust
// Send a request and await exactly one reply.
pub async fn request(
    &self,
    method: impl Into<String>,
    payload: BusPayload,
) -> Result<BusResult, BusCallError>

// Send a notification. Non-blocking (try_send). Lossy under back-pressure.
pub fn notify(
    &self,
    method: impl Into<String>,
    payload: BusPayload,
) -> Result<(), BusCallError>
```

`BusCallError` variants:
- `Send` — supervisor `mpsc` receiver was dropped (supervisor is dead)
- `Recv` — supervisor dropped `reply_tx` without sending a reply
- `Full` — notification dropped due to back-pressure (only from `notify`)

---

## `BusHandler` registration contract

Each subsystem implements `BusHandler` and registers with the supervisor at startup:

```rust
pub trait BusHandler: Send + Sync {
    fn prefix(&self) -> &str;
    fn handle_request(&self, method: &str, payload: BusPayload, reply_tx: oneshot::Sender<BusResult>);
    fn handle_notification(&self, _method: &str, _payload: BusPayload) {}  // default: no-op
}
```

Rules:
- `prefix()` must be unique. The supervisor **panics at startup** if two handlers share the same prefix.
- `handle_request` receives the **full method string** (e.g. `"agents/echo/handle"`), not just the suffix.
- `handle_notification` has a default no-op; subsystems that don't use notifications need not override it.
- Neither method may block the caller — offload async work to `tokio::spawn`.

The supervisor owns handlers as `Vec<Box<dyn BusHandler>>` and builds a `HashMap<&str, usize>` index at startup for O(1) prefix lookup.

---

## Management routes (`manage/*`)

The management subsystem owns the `manage` prefix. These routes are used by HTTP channels and control/CLI consumers. **No private or secure data** is exposed on the HTTP-facing routes.

| Method | Payload | Response | Use |
|--------|---------|----------|-----|
| `manage/http/get` | `Empty` | `CommsMessage` with health/status JSON | HTTP `GET /api/health` |
| `manage/http/tree` | `Empty` | `CommsMessage` with component tree JSON | HTTP `GET /api/tree` (same shape as `manage/tree`; no private data) |
| `manage/tree` | `Empty` | `CommsMessage` with component tree JSON | Control/CLI (e.g. `araliya-ctl tree`, `/tree`) |

**Tree JSON shape** (for both `manage/tree` and `manage/http/tree`): root object with `id`, `name`, `status`, `state`, `uptime_ms`, and `children` (array of nodes with `id`, `name`, `status`, `state`, `children`). All nodes use `status: "running"` and `state: "on"` for currently registered components.

---
TODO: check this section, code and doc
## Observability

The bus and supervisor emit structured `tracing` logs at `debug` and `trace` levels:

| Component | Level | What is logged |
|-----------|-------|----------------|
| `BusHandle::request` | `debug` | Request sent (id, method), request completed (id, ok/err) |
| `BusHandle::request` | `trace` | Full outbound payload, full result payload |
| `BusHandle::request` | `warn` | Send failure (supervisor dead), reply channel dropped |
| `BusHandle::notify` | `debug` | Notification sent (method) |
| `BusHandle::notify` | `trace` | Full notification payload |
| `BusHandle::notify` | `warn` | Buffer full (back-pressure), send failure |
| `SupervisorBus::new` | `debug` | Bus created with buffer size |
| Supervisor loop | `debug` | Handler registration, request/notification routing (id, method, prefix) |
| Supervisor loop | `trace` | Full request/notification payloads at dispatch time |
| Supervisor loop | `warn` | Unhandled request method (with error reply) |

Use `-vvv` (debug) for flow diagnostics, `-vvvv` (trace) for full payload inspection. See [Configuration](../../configuration.md#cli-verbosity-flags) for the full CLI flag table.

---

## IPC migration path

When the architecture is extended to cross a process boundary:

1. Remove `reply_tx` from `BusMessage::Request`.
2. The supervisor stores pending reply senders in a `HashMap<Uuid, oneshot::Sender<BusResult>>`.
3. Serialize `{ id, method, payload }` as JSON over a Unix socket or pipe.
4. Match responses back by `id` and resolve the stored sender.

`BusHandle::request()` is unchanged from callers' perspective — the async/await contract is identical.
