[supervisor]
bot_name = "araliya"
work_dir = "~/.araliya"
# Optional explicit identity directory.
# Absolute path or relative to work_dir (example: "bot-pkey51aee87e").
# Useful when multiple bot-pkey* directories exist.
# identity_dir = "bot-pkey51aee87e"
log_level = "info"
# Force supervisor stdio management adapter in interactive terminals.
# When true, PTY channel is auto-disabled and slash commands (/chat, /status, ...)
# are handled by the stdio management adapter.
stdio_management_interactive = true

[comms.pty]
# PTY (console) channel.
# If management stdio adapter claims stdin/stdout, PTY is auto-disabled and
# /chat is routed through a virtual PTY stream.
enabled = true

[comms.telegram]
# Telegram channel — requires TELEGRAM_BOT_TOKEN env var.
enabled = false # *AI AGENT: DO NOT CHANGE*

[comms.http]
# HTTP channel — API routes under /api/ (e.g. GET /api/health) and,
# when the UI subsystem is enabled, static UI serving on all other paths.
enabled = true
bind = "127.0.0.1:8080"

[agents]
# Which agent handles unrouted messages.
default = "chat"

[agents.routing]
# Optional channel_id -> agent_id overrides.
# Example:
# pty0 = "echo"

[agents.echo]
# Built-in echo agent. Reflects messages back verbatim.

[agents.basic_chat]
# Default conversational agent — routes messages to the active LLM provider.

[agents.chat]
# Session-aware chat agent — extends basic_chat with session management.
memory = ["basic_session"]

[agents.dummy]
# Disabled — placeholder to show the section shape.
enabled = false

[memory]
# Global memory subsystem configuration.

[memory.basic_session]
# kv_cap = 200
# transcript_cap = 500

[llm]
# Which LLM component to activate. Each component has its own config section below.
# Options: "dummy", "openai"
default = "openai"

[ui.svui]
# Svelte-based web UI backend — served by the HTTP channel.
# When enabled, non-API HTTP requests are delegated to this backend.
enabled = true
# Path to the static build directory (Svelte output from ui/svui).
# Relative to the working directory where the bot binary runs.
# If the directory doesn't exist, a built-in placeholder page is served.
static_dir = "ui/build"

[llm.dummy]
# No config — echoes input back as "[echo] {input}". Useful for testing.

[llm.openai]
# Used when default = "openai". API key must be set via LLM_API_KEY env var (never in TOML).
# "openai-compatible" also uses this section (e.g. for local Ollama / LM Studio endpoints).
api_base_url = "https://api.openai.com/v1/chat/completions"
model = "gpt-5-nano"
temperature = 0.2
timeout_seconds = 60
