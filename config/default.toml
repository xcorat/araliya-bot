[supervisor]
bot_name = "araliya"
work_dir = "~/.araliya"
# Optional explicit identity directory.
# Absolute path or relative to work_dir (example: "bot-pkey51aee87e").
# Useful when multiple bot-pkey* directories exist.
# identity_dir = "bot-pkey51aee87e"
log_level = "info"

[comms.pty]
# PTY (console) channel — auto-loads when no other channel is enabled.
# Set to false to suppress even when no other channel is present.
enabled = true

[comms.telegram]
# Telegram channel — requires TELEGRAM_BOT_TOKEN env var.
enabled = true

[agents]
# Which agent handles unrouted messages.
default = "basic_chat"

[agents.routing]
# Optional channel_id -> agent_id overrides.
# Example:
# pty0 = "echo"

[agents.echo]
# Built-in echo agent. Reflects messages back verbatim.

[agents.basic_chat]
# Default conversational agent — routes messages to the active LLM provider.

[agents.chat]
# Session-aware chat agent — extends basic_chat with session management.
memory = ["basic_session"]

[agents.dummy]
# Disabled — placeholder to show the section shape.
enabled = false

[memory]
# Global memory subsystem configuration.

[memory.basic_session]
# kv_cap = 200
# transcript_cap = 500

[llm]
# Which LLM component to activate. Each component has its own config section below.
# Options: "dummy", "openai"
default = "openai"

[llm.dummy]
# No config — echoes input back as "[echo] {input}". Useful for testing.

[llm.openai]
# Used when default = "openai". API key must be set via LLM_API_KEY env var (never in TOML).
# "openai-compatible" also uses this section (e.g. for local Ollama / LM Studio endpoints).
api_base_url = "https://api.openai.com/v1/chat/completions"
model = "gpt-4o-mini"
temperature = 0.2
timeout_seconds = 60
